{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "password=getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_COLAB=0\n",
      "Warning: Not a Colab Environment\n",
      "env: TAO_DOCKER_DISABLE=0\n",
      "env: KEY=nvidia_tao\n",
      "env: NUM_GPUS=1\n",
      "env: COLAB_NOTEBOOKS_PATH=/root/TAO/nvidia-tao\n",
      "env: EXPERIMENT_DIR=/root/TAO/results\n",
      "env: DATA_DIR=/root/TAO/GenderClassification/data\n",
      "env: SPECS_DIR=/root/TAO/nvidia-tao/tensorflow/classification/specs\n",
      "total 8\n",
      "-rwxrwxrwx 1 1000 1000 1262 Jan 16 13:21 classification_spec.cfg\n",
      "-rwxrwxrwx 1 1000 1000 1121 Jan 17 05:47 classification_retrain_spec.cfg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env GOOGLE_COLAB=0\n",
    "print(\"Warning: Not a Colab Environment\")\n",
    "\n",
    "\n",
    "# Setting up env variables for cleaner command line commands.\n",
    "import os\n",
    "\n",
    "%env TAO_DOCKER_DISABLE=0\n",
    "\n",
    "%env KEY=nvidia_tao\n",
    "#FIXME1\n",
    "%env NUM_GPUS=1\n",
    "\n",
    "#FIXME2\n",
    "%env COLAB_NOTEBOOKS_PATH=/root/TAO/nvidia-tao\n",
    "\n",
    "if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n",
    "    !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n",
    "\n",
    "#FIXME3\n",
    "%env EXPERIMENT_DIR=/root/TAO/results\n",
    "#FIXME4\n",
    "delete_existing_experiments = False\n",
    "#FIXME5\n",
    "%env DATA_DIR=/root/TAO/GenderClassification/data\n",
    "#FIXME6\n",
    "delete_existing_data = False\n",
    "\n",
    "if delete_existing_experiments:\n",
    "    os.system(f'echo {password} | sudo -S rm -rf $EXPERIMENT_DIR')\n",
    "if delete_existing_data:\n",
    "    os.system(f'echo {password} | sudo -S rm -rf $DATA_DIR')\n",
    "\n",
    "SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/classification/specs\"\n",
    "%env SPECS_DIR={SPECS_DIR}\n",
    "# Showing list of specification files.\n",
    "!ls -rlt $SPECS_DIR\n",
    "\n",
    "os.system(f'echo {password} | sudo -S mkdir -p $DATA_DIR && echo {password} | sudo -S chmod -R 777 $DATA_DIR')\n",
    "os.system(f'echo {password} | sudo -S mkdir -p $EXPERIMENT_DIR && echo {password} | sudo -S chmod -R 777 $EXPERIMENT_DIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preperation of Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 58658 files [00:04, 14447.89 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders as sf\n",
    "\n",
    "sf.ratio('data/data1',output='data/split',ratio=(0.8,0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOCAL_PROJECT_DIR=/ngc_content/\n",
      "env: CLI=ngccli_cat_linux.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for fkurt: [sudo] password for fkurt: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-16 14:55:27--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 52.84.106.100, 52.84.106.123, 52.84.106.124, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|52.84.106.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39270256 (37M) [application/zip]\n",
      "Saving to: ‘/ngc_content//ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  37,45M  10,6MB/s    in 3,5s    \n",
      "\n",
      "2023-01-16 14:55:32 (10,6 MB/s) - ‘/ngc_content//ngccli/ngccli_cat_linux.zip’ saved [39270256/39270256]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env LOCAL_PROJECT_DIR=/ngc_content/\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "os.system(f'echo {password} | sudo -S mkdir -p $LOCAL_PROJECT_DIR/ngccli && echo {password} | sudo -S chmod -R 777 $LOCAL_PROJECT_DIR')\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "os.system(f'echo {password} | sudo -S rm -rf $LOCAL_PROJECT_DIR/ngccli/*')\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n",
    "!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| Versi | Accur | Epoch | Batch | GPU   | Memor | File  | Statu | Creat |\n",
      "| on    | acy   | s     | Size  | Model | y Foo | Size  | s     | ed    |\n",
      "|       |       |       |       |       | tprin |       |       | Date  |\n",
      "|       |       |       |       |       | t     |       |       |       |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "| vgg19 | 77.56 | 80    | 1     | V100  | 153.7 | 153.7 | UPLOA | Aug   |\n",
      "|       |       |       |       |       |       | 2 MB  | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| vgg16 | 77.17 | 80    | 1     | V100  | 113.2 | 113.1 | UPLOA | Aug   |\n",
      "|       |       |       |       |       |       | 6 MB  | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| squee | 65.13 | 80    | 1     | V100  | 6.5   | 6.46  | UPLOA | Aug   |\n",
      "| zenet |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| resne | 77.91 | 80    | 1     | V100  | 294.2 | 294.2 | UPLOA | Aug   |\n",
      "| t50   |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| resne | 77.04 | 80    | 1     | V100  | 170.7 | 170.6 | UPLOA | Aug   |\n",
      "| t34   |       |       |       |       |       | 5 MB  | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| resne | 76.74 | 80    | 1     | V100  | 89.0  | 88.96 | UPLOA | Aug   |\n",
      "| t18   |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| resne | 77.78 | 80    | 1     | V100  | 576.3 | 576.3 | UPLOA | Aug   |\n",
      "| t101  |       |       |       |       |       | 3 MB  | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| resne | 74.38 | 80    | 1     | V100  | 38.3  | 38.31 | UPLOA | Aug   |\n",
      "| t10   |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| mobil | 72.75 | 80    | 1     | V100  | 5.0   | 5.01  | UPLOA | Aug   |\n",
      "| enet_ |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "| v2    |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| mobil | 79.5  | 80    | 1     | V100  | 26.2  | 26.22 | UPLOA | Aug   |\n",
      "| enet_ |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "| v1    |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| googl | 77.11 | 80    | 1     | V100  | 47.6  | 47.64 | UPLOA | Aug   |\n",
      "| enet  |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| effic | 77.11 | 80    | 1     | V100  | 26.8  | 26.78 | UPLOA | Aug   |\n",
      "| ientn |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "| et_b1 |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| _swis |       |       |       |       |       |       |       |       |\n",
      "| h     |       |       |       |       |       |       |       |       |\n",
      "| effic | 77.11 | 80    | 1     | V100  | 26.8  | 26.78 | UPLOA | Aug   |\n",
      "| ientn |       |       |       |       |       | MB    | D_COM | 18,   |\n",
      "| et_b1 |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| _relu |       |       |       |       |       |       |       |       |\n",
      "| darkn | 76.44 | 80    | 1     | V100  | 311.7 | 311.6 | UPLOA | Aug   |\n",
      "| et53  |       |       |       |       |       | 8 MB  | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| darkn | 77.52 | 80    | 1     | V100  | 152.8 | 152.8 | UPLOA | Aug   |\n",
      "| et19  |       |       |       |       |       | 2 MB  | D_COM | 18,   |\n",
      "|       |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| cspda | 77.1  | 80    | 1     | V100  | 28.6  | 28.57 | UPLOA | Nov   |\n",
      "| rknet |       |       |       |       |       | MB    | D_COM | 23,   |\n",
      "| _tiny |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| cspda | 76.44 | 80    | 1     | V100  | 103.0 | 102.9 | UPLOA | Sep   |\n",
      "| rknet |       |       |       |       |       | 9 MB  | D_COM | 10,   |\n",
      "| 53    |       |       |       |       |       |       | PLETE | 2021  |\n",
      "| cspda | 77.52 | 80    | 1     | V100  | 62.9  | 62.86 | UPLOA | Sep   |\n",
      "| rknet |       |       |       |       |       | MB    | D_COM | 10,   |\n",
      "| 19    |       |       |       |       |       |       | PLETE | 2021  |\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "!ngc registry model list nvidia/tao/pretrained_classification:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 82.38 MB in 6m 53s, Download speed: 203.98 KB/s               \n",
      "--------------------------------------------------------------------------------\n",
      "   Transfer id: pretrained_classification_vresnet18\n",
      "   Download status: Completed\n",
      "   Downloaded local path: /home/fkurt/Murat/TAO/results/gender_classification/pretrained_resnet18/pretrained_classification_vresnet18\n",
      "   Total files downloaded: 1\n",
      "   Total downloaded size: 82.38 MB\n",
      "   Started at: 2023-01-16 14:03:20.375468\n",
      "   Completed at: 2023-01-16 14:10:13.945094\n",
      "   Duration taken: 6m 53s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Pull pretrained model from NGC\n",
    "!ngc registry model download-version nvidia/tao/pretrained_classification:resnet18 --dest $EXPERIMENT_DIR/pretrained_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that model is downloaded into dir.\n",
      "total 91100\n",
      "-rwxrwxrwx 1 fkurt fkurt 93278448 Oca 16 14:10 resnet_18.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(\"Check that model is downloaded into dir.\")\n",
    "!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_classification_vresnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for fkurt: --2023-01-16 14:55:42--  https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4-Linux-x86_64.sh\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/fc11d880-7650-11e9-969f-7442127f007a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230116T115542Z&X-Amz-Expires=300&X-Amz-Signature=c5e62e86ab49e61d8a14088d812e54d3a9ac0e9d6bc63e28e8dd6e7aeeb8a86c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=537699&response-content-disposition=attachment%3B%20filename%3Dcmake-3.14.4-Linux-x86_64.sh&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-01-16 14:55:42--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/537699/fc11d880-7650-11e9-969f-7442127f007a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230116T115542Z&X-Amz-Expires=300&X-Amz-Signature=c5e62e86ab49e61d8a14088d812e54d3a9ac0e9d6bc63e28e8dd6e7aeeb8a86c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=537699&response-content-disposition=attachment%3B%20filename%3Dcmake-3.14.4-Linux-x86_64.sh&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37196929 (35M) [application/octet-stream]\n",
      "Saving to: ‘cmake-3.14.4-Linux-x86_64.sh’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1,99M 18s\n",
      "    50K .......... .......... .......... .......... ..........  0% 2,87M 15s\n",
      "   100K .......... .......... .......... .......... ..........  0% 9,12M 11s\n",
      "   150K .......... .......... .......... .......... ..........  0% 11,0M 9s\n",
      "   200K .......... .......... .......... .......... ..........  0% 4,41M 9s\n",
      "   250K .......... .......... .......... .......... ..........  0% 11,7M 8s\n",
      "   300K .......... .......... .......... .......... ..........  0% 11,1M 7s\n",
      "   350K .......... .......... .......... .......... ..........  1% 8,69M 7s\n",
      "   400K .......... .......... .......... .......... ..........  1% 11,7M 6s\n",
      "   450K .......... .......... .......... .......... ..........  1% 11,7M 6s\n",
      "   500K .......... .......... .......... .......... ..........  1%  565K 11s\n",
      "   550K .......... .......... .......... .......... ..........  1%  560M 10s\n",
      "   600K .......... .......... .......... .......... ..........  1%  656M 10s\n",
      "   650K .......... .......... .......... .......... ..........  1%  655M 9s\n",
      "   700K .......... .......... .......... .......... ..........  2%  659M 8s\n",
      "   750K .......... .......... .......... .......... ..........  2%  542M 8s\n",
      "   800K .......... .......... .......... .......... ..........  2%  678M 7s\n",
      "   850K .......... .......... .......... .......... ..........  2%  668M 7s\n",
      "   900K .......... .......... .......... .......... ..........  2%  681M 6s\n",
      "   950K .......... .......... .......... .......... ..........  2%  633M 6s\n",
      "  1000K .......... .......... .......... .......... ..........  2%  697M 6s\n",
      "  1050K .......... .......... .......... .......... ..........  3%  676M 6s\n",
      "  1100K .......... .......... .......... .......... ..........  3% 1,27M 7s\n",
      "  1150K .......... .......... .......... .......... ..........  3%  511M 6s\n",
      "  1200K .......... .......... .......... .......... ..........  3%  559M 6s\n",
      "  1250K .......... .......... .......... .......... ..........  3%  652M 6s\n",
      "  1300K .......... .......... .......... .......... ..........  3%  652M 6s\n",
      "  1350K .......... .......... .......... .......... ..........  3%  599M 5s\n",
      "  1400K .......... .......... .......... .......... ..........  3%  667M 5s\n",
      "  1450K .......... .......... .......... .......... ..........  4%  657M 5s\n",
      "  1500K .......... .......... .......... .......... ..........  4%  666M 5s\n",
      "  1550K .......... .......... .......... .......... ..........  4%  544M 5s\n",
      "  1600K .......... .......... .......... .......... ..........  4%  685M 4s\n",
      "  1650K .......... .......... .......... .......... ..........  4%  697M 4s\n",
      "  1700K .......... .......... .......... .......... ..........  4%  622M 4s\n",
      "  1750K .......... .......... .......... .......... ..........  4%  691M 4s\n",
      "  1800K .......... .......... .......... .......... ..........  5%  637M 4s\n",
      "  1850K .......... .......... .......... .......... ..........  5%  683M 4s\n",
      "  1900K .......... .......... .......... .......... ..........  5%  545M 4s\n",
      "  1950K .......... .......... .......... .......... ..........  5% 23,5M 4s\n",
      "  2000K .......... .......... .......... .......... ..........  5% 11,8M 4s\n",
      "  2050K .......... .......... .......... .......... ..........  5% 11,1M 4s\n",
      "  2100K .......... .......... .......... .......... ..........  5% 11,7M 4s\n",
      "  2150K .......... .......... .......... .......... ..........  6% 11,7M 4s\n",
      "  2200K .......... .......... .......... .......... ..........  6% 11,7M 4s\n",
      "  2250K .......... .......... .......... .......... ..........  6% 11,8M 4s\n",
      "  2300K .......... .......... .......... .......... ..........  6% 11,7M 4s\n",
      "  2350K .......... .......... .......... .......... ..........  6% 8,69M 4s\n",
      "  2400K .......... .......... .......... .......... ..........  6% 11,1M 4s\n",
      "  2450K .......... .......... .......... .......... ..........  6% 12,5M 4s\n",
      "  2500K .......... .......... .......... .......... ..........  7% 11,6M 3s\n",
      "  2550K .......... .......... .......... .......... ..........  7% 11,2M 3s\n",
      "  2600K .......... .......... .......... .......... ..........  7% 11,7M 3s\n",
      "  2650K .......... .......... .......... .......... ..........  7% 11,8M 3s\n",
      "  2700K .......... .......... .......... .......... ..........  7% 11,7M 3s\n",
      "  2750K .......... .......... .......... .......... ..........  7% 9,07M 3s\n",
      "  2800K .......... .......... .......... .......... ..........  7% 11,0M 3s\n",
      "  2850K .......... .......... .......... .......... ..........  7% 11,9M 3s\n",
      "  2900K .......... .......... .......... .......... ..........  8% 11,7M 3s\n",
      "  2950K .......... .......... .......... .......... ..........  8% 1,21M 4s\n",
      "  3000K .......... .......... .......... .......... ..........  8%  594M 4s\n",
      "  3050K .......... .......... .......... .......... ..........  8%  653M 4s\n",
      "  3100K .......... .......... .......... .......... ..........  8%  583M 4s\n",
      "  3150K .......... .......... .......... .......... ..........  8%  547M 4s\n",
      "  3200K .......... .......... .......... .......... ..........  8%  689M 3s\n",
      "  3250K .......... .......... .......... .......... ..........  9%  605M 3s\n",
      "  3300K .......... .......... .......... .......... ..........  9%  673M 3s\n",
      "  3350K .......... .......... .......... .......... ..........  9%  626M 3s\n",
      "  3400K .......... .......... .......... .......... ..........  9% 27,4M 3s\n",
      "  3450K .......... .......... .......... .......... ..........  9% 11,1M 3s\n",
      "  3500K .......... .......... .......... .......... ..........  9% 8,68M 3s\n",
      "  3550K .......... .......... .......... .......... ..........  9% 11,6M 3s\n",
      "  3600K .......... .......... .......... .......... .......... 10% 11,7M 3s\n",
      "  3650K .......... .......... .......... .......... .......... 10% 11,8M 3s\n",
      "  3700K .......... .......... .......... .......... .......... 10% 11,7M 3s\n",
      "  3750K .......... .......... .......... .......... .......... 10% 11,7M 3s\n",
      "  3800K .......... .......... .......... .......... .......... 10% 11,7M 3s\n",
      "  3850K .......... .......... .......... .......... .......... 10% 11,7M 3s\n",
      "  3900K .......... .......... .......... .......... .......... 10% 8,69M 3s\n",
      "  3950K .......... .......... .......... .......... .......... 11% 1,55M 3s\n",
      "  4000K .......... .......... .......... .......... .......... 11%  161M 3s\n",
      "  4050K .......... .......... .......... .......... .......... 11%  599M 3s\n",
      "  4100K .......... .......... .......... .......... .......... 11%  721M 3s\n",
      "  4150K .......... .......... .......... .......... .......... 11%  747M 3s\n",
      "  4200K .......... .......... .......... .......... .......... 11%  675M 3s\n",
      "  4250K .......... .......... .......... .......... .......... 11%  755M 3s\n",
      "  4300K .......... .......... .......... .......... .......... 11% 27,0M 3s\n",
      "  4350K .......... .......... .......... .......... .......... 12% 10,6M 3s\n",
      "  4400K .......... .......... .......... .......... .......... 12% 4,64M 3s\n",
      "  4450K .......... .......... .......... .......... .......... 12% 1,80M 3s\n",
      "  4500K .......... .......... .......... .......... .......... 12%  694M 3s\n",
      "  4550K .......... .......... .......... .......... .......... 12%  740M 3s\n",
      "  4600K .......... .......... .......... .......... .......... 12%  679M 3s\n",
      "  4650K .......... .......... .......... .......... .......... 12%  778M 3s\n",
      "  4700K .......... .......... .......... .......... .......... 13% 11,9M 3s\n",
      "  4750K .......... .......... .......... .......... .......... 13% 5,41M 3s\n",
      "  4800K .......... .......... .......... .......... .......... 13% 7,53M 3s\n",
      "  4850K .......... .......... .......... .......... .......... 13% 9,05M 3s\n",
      "  4900K .......... .......... .......... .......... .......... 13% 5,32M 3s\n",
      "  4950K .......... .......... .......... .......... .......... 13% 6,79M 3s\n",
      "  5000K .......... .......... .......... .......... .......... 13% 8,76M 3s\n",
      "  5050K .......... .......... .......... .......... .......... 14% 6,13M 3s\n",
      "  5100K .......... .......... .......... .......... .......... 14% 5,47M 3s\n",
      "  5150K .......... .......... .......... .......... .......... 14% 5,89M 3s\n",
      "  5200K .......... .......... .......... .......... .......... 14% 1,91M 3s\n",
      "  5250K .......... .......... .......... .......... .......... 14%  595M 3s\n",
      "  5300K .......... .......... .......... .......... .......... 14%  610M 3s\n",
      "  5350K .......... .......... .......... .......... .......... 14% 12,9M 3s\n",
      "  5400K .......... .......... .......... .......... .......... 15% 4,25M 3s\n",
      "  5450K .......... .......... .......... .......... .......... 15% 6,05M 3s\n",
      "  5500K .......... .......... .......... .......... .......... 15% 3,40M 3s\n",
      "  5550K .......... .......... .......... .......... .......... 15% 5,42M 3s\n",
      "  5600K .......... .......... .......... .......... .......... 15% 4,63M 3s\n",
      "  5650K .......... .......... .......... .......... .......... 15% 5,45M 3s\n",
      "  5700K .......... .......... .......... .......... .......... 15% 4,82M 3s\n",
      "  5750K .......... .......... .......... .......... .......... 15% 5,16M 3s\n",
      "  5800K .......... .......... .......... .......... .......... 16% 4,88M 3s\n",
      "  5850K .......... .......... .......... .......... .......... 16% 5,40M 3s\n",
      "  5900K .......... .......... .......... .......... .......... 16% 3,80M 3s\n",
      "  5950K .......... .......... .......... .......... .......... 16% 5,47M 3s\n",
      "  6000K .......... .......... .......... .......... .......... 16% 4,91M 3s\n",
      "  6050K .......... .......... .......... .......... .......... 16% 4,59M 4s\n",
      "  6100K .......... .......... .......... .......... .......... 16% 1,68M 4s\n",
      "  6150K .......... .......... .......... .......... .......... 17%  593M 4s\n",
      "  6200K .......... .......... .......... .......... .......... 17% 2,63M 4s\n",
      "  6250K .......... .......... .......... .......... .......... 17%  638M 4s\n",
      "  6300K .......... .......... .......... .......... .......... 17% 4,81M 4s\n",
      "  6350K .......... .......... .......... .......... .......... 17% 3,67M 4s\n",
      "  6400K .......... .......... .......... .......... .......... 17% 2,67M 4s\n",
      "  6450K .......... .......... .......... .......... .......... 17% 3,53M 4s\n",
      "  6500K .......... .......... .......... .......... .......... 18% 3,51M 4s\n",
      "  6550K .......... .......... .......... .......... .......... 18% 1,36M 4s\n",
      "  6600K .......... .......... .......... .......... .......... 18%  510M 4s\n",
      "  6650K .......... .......... .......... .......... .......... 18% 5,22M 4s\n",
      "  6700K .......... .......... .......... .......... .......... 18% 2,33M 4s\n",
      "  6750K .......... .......... .......... .......... .......... 18% 2,54M 4s\n",
      "  6800K .......... .......... .......... .......... .......... 18% 1,82M 4s\n",
      "  6850K .......... .......... .......... .......... .......... 18% 2,91M 4s\n",
      "  6900K .......... .......... .......... .......... .......... 19% 2,57M 4s\n",
      "  6950K .......... .......... .......... .......... .......... 19% 2,76M 4s\n",
      "  7000K .......... .......... .......... .......... .......... 19% 2,59M 4s\n",
      "  7050K .......... .......... .......... .......... .......... 19% 2,73M 4s\n",
      "  7100K .......... .......... .......... .......... .......... 19% 2,70M 4s\n",
      "  7150K .......... .......... .......... .......... .......... 19% 2,69M 4s\n",
      "  7200K .......... .......... .......... .......... .......... 19% 2,13M 4s\n",
      "  7250K .......... .......... .......... .......... .......... 20% 2,59M 4s\n",
      "  7300K .......... .......... .......... .......... .......... 20% 3,12M 4s\n",
      "  7350K .......... .......... .......... .......... .......... 20% 1,27M 5s\n",
      "  7400K .......... .......... .......... .......... .......... 20%  463M 4s\n",
      "  7450K .......... .......... .......... .......... .......... 20% 2,49M 5s\n",
      "  7500K .......... .......... .......... .......... .......... 20% 1,97M 5s\n",
      "  7550K .......... .......... .......... .......... .......... 20% 2,23M 5s\n",
      "  7600K .......... .......... .......... .......... .......... 21% 1,53M 5s\n",
      "  7650K .......... .......... .......... .......... .......... 21% 2,15M 5s\n",
      "  7700K .......... .......... .......... .......... .......... 21% 1,18M 5s\n",
      "  7750K .......... .......... .......... .......... .......... 21% 2,18M 5s\n",
      "  7800K .......... .......... .......... .......... .......... 21% 1,37M 5s\n",
      "  7850K .......... .......... .......... .......... .......... 21% 1,55M 5s\n",
      "  7900K .......... .......... .......... .......... .......... 21% 1,80M 5s\n",
      "  7950K .......... .......... .......... .......... .......... 22% 1,43M 5s\n",
      "  8000K .......... .......... .......... .......... .......... 22% 1,35M 5s\n",
      "  8050K .......... .......... .......... .......... .......... 22% 1,69M 5s\n",
      "  8100K .......... .......... .......... .......... .......... 22% 1,88M 5s\n",
      "  8150K .......... .......... .......... .......... .......... 22% 1,08M 6s\n",
      "  8200K .......... .......... .......... .......... .......... 22% 3,21M 6s\n",
      "  8250K .......... .......... .......... .......... .......... 22% 1,36M 6s\n",
      "  8300K .......... .......... .......... .......... .......... 22% 1,37M 6s\n",
      "  8350K .......... .......... .......... .......... .......... 23% 1,36M 6s\n",
      "  8400K .......... .......... .......... .......... .......... 23% 1,04M 6s\n",
      "  8450K .......... .......... .......... .......... .......... 23% 1,53M 6s\n",
      "  8500K .......... .......... .......... .......... .......... 23% 1,48M 6s\n",
      "  8550K .......... .......... .......... .......... .......... 23% 1,58M 6s\n",
      "  8600K .......... .......... .......... .......... .......... 23% 1,35M 6s\n",
      "  8650K .......... .......... .......... .......... .......... 23%  978K 6s\n",
      "  8700K .......... .......... .......... .......... .......... 24% 1,60M 6s\n",
      "  8750K .......... .......... .......... .......... .......... 24% 1,18M 6s\n",
      "  8800K .......... .......... .......... .......... .......... 24%  970K 6s\n",
      "  8850K .......... .......... .......... .......... .......... 24% 1,49M 7s\n",
      "  8900K .......... .......... .......... .......... .......... 24% 1,32M 7s\n",
      "  8950K .......... .......... .......... .......... .......... 24% 1,53M 7s\n",
      "  9000K .......... .......... .......... .......... .......... 24% 1,48M 7s\n",
      "  9050K .......... .......... .......... .......... .......... 25% 1,50M 7s\n",
      "  9100K .......... .......... .......... .......... .......... 25% 1,56M 7s\n",
      "  9150K .......... .......... .......... .......... .......... 25% 1,83M 7s\n",
      "  9200K .......... .......... .......... .......... .......... 25% 1,27M 7s\n",
      "  9250K .......... .......... .......... .......... .......... 25% 1,79M 7s\n",
      "  9300K .......... .......... .......... .......... .......... 25% 1,75M 7s\n",
      "  9350K .......... .......... .......... .......... .......... 25% 2,05M 7s\n",
      "  9400K .......... .......... .......... .......... .......... 26% 1,83M 7s\n",
      "  9450K .......... .......... .......... .......... .......... 26% 2,19M 7s\n",
      "  9500K .......... .......... .......... .......... .......... 26% 1,40M 7s\n",
      "  9550K .......... .......... .......... .......... .......... 26% 2,16M 7s\n",
      "  9600K .......... .......... .......... .......... .......... 26%  985K 7s\n",
      "  9650K .......... .......... .......... .......... .......... 26% 1002K 7s\n",
      "  9700K .......... .......... .......... .......... .......... 26% 1,02M 7s\n",
      "  9750K .......... .......... .......... .......... .......... 26% 1,16M 7s\n",
      "  9800K .......... .......... .......... .......... .......... 27% 1,23M 7s\n",
      "  9850K .......... .......... .......... .......... .......... 27% 1,24M 7s\n",
      "  9900K .......... .......... .......... .......... .......... 27% 1,27M 8s\n",
      "  9950K .......... .......... .......... .......... .......... 27% 1,43M 8s\n",
      " 10000K .......... .......... .......... .......... .......... 27%  748K 8s\n",
      " 10050K .......... .......... .......... .......... .......... 27% 1,08M 8s\n",
      " 10100K .......... .......... .......... .......... .......... 27% 1,10M 8s\n",
      " 10150K .......... .......... .......... .......... .......... 28% 1,13M 8s\n",
      " 10200K .......... .......... .......... .......... .......... 28% 1,21M 8s\n",
      " 10250K .......... .......... .......... .......... .......... 28% 1,24M 8s\n",
      " 10300K .......... .......... .......... .......... .......... 28% 1,31M 8s\n",
      " 10350K .......... .......... .......... .......... .......... 28% 1,45M 8s\n",
      " 10400K .......... .......... .......... .......... .......... 28% 1,07M 8s\n",
      " 10450K .......... .......... .......... .......... .......... 28% 1,45M 8s\n",
      " 10500K .......... .......... .......... .......... .......... 29% 1,68M 8s\n",
      " 10550K .......... .......... .......... .......... .......... 29% 1,56M 8s\n",
      " 10600K .......... .......... .......... .......... .......... 29% 1,11M 8s\n",
      " 10650K .......... .......... .......... .......... .......... 29% 1,90M 8s\n",
      " 10700K .......... .......... .......... .......... .......... 29% 1,24M 8s\n",
      " 10750K .......... .......... .......... .......... .......... 29%  726K 8s\n",
      " 10800K .......... .......... .......... .......... .......... 29%  990K 8s\n",
      " 10850K .......... .......... .......... .......... .......... 30%  979K 9s\n",
      " 10900K .......... .......... .......... .......... .......... 30% 1002K 9s\n",
      " 10950K .......... .......... .......... .......... .......... 30% 1,10M 9s\n",
      " 11000K .......... .......... .......... .......... .......... 30% 1,15M 9s\n",
      " 11050K .......... .......... .......... .......... .......... 30% 1,21M 9s\n",
      " 11100K .......... .......... .......... .......... .......... 30% 1,24M 9s\n",
      " 11150K .......... .......... .......... .......... .......... 30% 1,37M 9s\n",
      " 11200K .......... .......... .......... .......... .......... 30%  818K 9s\n",
      " 11250K .......... .......... .......... .......... .......... 31% 1,68M 9s\n",
      " 11300K .......... .......... .......... .......... .......... 31% 1023K 9s\n",
      " 11350K .......... .......... .......... .......... .......... 31% 1,05M 9s\n",
      " 11400K .......... .......... .......... .......... .......... 31% 1,14M 9s\n",
      " 11450K .......... .......... .......... .......... .......... 31%  975K 9s\n",
      " 11500K .......... .......... .......... .......... .......... 31%  746K 9s\n",
      " 11550K .......... .......... .......... .......... .......... 31%  650K 9s\n",
      " 11600K .......... .......... .......... .......... .......... 32%  583K 9s\n",
      " 11650K .......... .......... .......... .......... .......... 32%  849K 9s\n",
      " 11700K .......... .......... .......... .......... .......... 32%  725K 9s\n",
      " 11750K .......... .......... .......... .......... .......... 32% 1021K 10s\n",
      " 11800K .......... .......... .......... .......... .......... 32% 1,01M 10s\n",
      " 11850K .......... .......... .......... .......... .......... 32% 1,16M 10s\n",
      " 11900K .......... .......... .......... .......... .......... 32% 1,11M 10s\n",
      " 11950K .......... .......... .......... .......... .......... 33% 1,20M 10s\n",
      " 12000K .......... .......... .......... .......... .......... 33%  764K 10s\n",
      " 12050K .......... .......... .......... .......... .......... 33% 1,01M 10s\n",
      " 12100K .......... .......... .......... .......... .......... 33%  959K 10s\n",
      " 12150K .......... .......... .......... .......... .......... 33% 1,17M 10s\n",
      " 12200K .......... .......... .......... .......... .......... 33% 1,06M 10s\n",
      " 12250K .......... .......... .......... .......... .......... 33% 1,24M 10s\n",
      " 12300K .......... .......... .......... .......... .......... 33% 1,39M 10s\n",
      " 12350K .......... .......... .......... .......... .......... 34% 1,31M 10s\n",
      " 12400K .......... .......... .......... .......... .......... 34% 1013K 10s\n",
      " 12450K .......... .......... .......... .......... .......... 34% 1,53M 10s\n",
      " 12500K .......... .......... .......... .......... .......... 34% 1,49M 10s\n",
      " 12550K .......... .......... .......... .......... .......... 34% 1,64M 10s\n",
      " 12600K .......... .......... .......... .......... .......... 34% 1,43M 10s\n",
      " 12650K .......... .......... .......... .......... .......... 34%  832K 10s\n",
      " 12700K .......... .......... .......... .......... .......... 35%  638M 10s\n",
      " 12750K .......... .......... .......... .......... .......... 35% 1,20M 10s\n",
      " 12800K .......... .......... .......... .......... .......... 35%  987K 10s\n",
      " 12850K .......... .......... .......... .......... .......... 35% 1,26M 10s\n",
      " 12900K .......... .......... .......... .......... .......... 35% 1,23M 10s\n",
      " 12950K .......... .......... .......... .......... .......... 35% 1,50M 10s\n",
      " 13000K .......... .......... .......... .......... .......... 35%  957K 10s\n",
      " 13050K .......... .......... .......... .......... .......... 36% 1,65M 10s\n",
      " 13100K .......... .......... .......... .......... .......... 36% 1,04M 10s\n",
      " 13150K .......... .......... .......... .......... .......... 36% 1,19M 10s\n",
      " 13200K .......... .......... .......... .......... .......... 36%  958K 10s\n",
      " 13250K .......... .......... .......... .......... .......... 36% 1,22M 10s\n",
      " 13300K .......... .......... .......... .......... .......... 36% 1,36M 10s\n",
      " 13350K .......... .......... .......... .......... .......... 36% 1,42M 10s\n",
      " 13400K .......... .......... .......... .......... .......... 37% 1,58M 10s\n",
      " 13450K .......... .......... .......... .......... .......... 37% 1,44M 10s\n",
      " 13500K .......... .......... .......... .......... .......... 37% 1,60M 10s\n",
      " 13550K .......... .......... .......... .......... .......... 37% 1,60M 10s\n",
      " 13600K .......... .......... .......... .......... .......... 37% 1,37M 10s\n",
      " 13650K .......... .......... .......... .......... .......... 37% 1,77M 10s\n",
      " 13700K .......... .......... .......... .......... .......... 37% 1,77M 10s\n",
      " 13750K .......... .......... .......... .......... .......... 37% 2,00M 10s\n",
      " 13800K .......... .......... .......... .......... .......... 38% 1,91M 10s\n",
      " 13850K .......... .......... .......... .......... .......... 38% 1,80M 10s\n",
      " 13900K .......... .......... .......... .......... .......... 38% 2,07M 10s\n",
      " 13950K .......... .......... .......... .......... .......... 38%  978K 10s\n",
      " 14000K .......... .......... .......... .......... .......... 38% 3,64M 10s\n",
      " 14050K .......... .......... .......... .......... .......... 38% 1,28M 10s\n",
      " 14100K .......... .......... .......... .......... .......... 38% 1,41M 10s\n",
      " 14150K .......... .......... .......... .......... .......... 39% 1,51M 10s\n",
      " 14200K .......... .......... .......... .......... .......... 39% 1,53M 10s\n",
      " 14250K .......... .......... .......... .......... .......... 39% 1,61M 10s\n",
      " 14300K .......... .......... .......... .......... .......... 39% 1,61M 10s\n",
      " 14350K .......... .......... .......... .......... .......... 39% 1,73M 10s\n",
      " 14400K .......... .......... .......... .......... .......... 39%  841K 10s\n",
      " 14450K .......... .......... .......... .......... .......... 39% 4,65M 10s\n",
      " 14500K .......... .......... .......... .......... .......... 40% 1,22M 10s\n",
      " 14550K .......... .......... .......... .......... .......... 40% 1,02M 10s\n",
      " 14600K .......... .......... .......... .......... .......... 40%  930K 10s\n",
      " 14650K .......... .......... .......... .......... .......... 40% 1006K 10s\n",
      " 14700K .......... .......... .......... .......... .......... 40%  983K 10s\n",
      " 14750K .......... .......... .......... .......... .......... 40% 1,08M 10s\n",
      " 14800K .......... .......... .......... .......... .......... 40%  617K 10s\n",
      " 14850K .......... .......... .......... .......... .......... 41%  857K 10s\n",
      " 14900K .......... .......... .......... .......... .......... 41%  873K 10s\n",
      " 14950K .......... .......... .......... .......... .......... 41% 1018K 10s\n",
      " 15000K .......... .......... .......... .......... .......... 41% 1,01M 10s\n",
      " 15050K .......... .......... .......... .......... .......... 41% 1,07M 10s\n",
      " 15100K .......... .......... .......... .......... .......... 41% 1,17M 10s\n",
      " 15150K .......... .......... .......... .......... .......... 41% 1,25M 10s\n",
      " 15200K .......... .......... .......... .......... .......... 41%  982K 10s\n",
      " 15250K .......... .......... .......... .......... .......... 42% 1,50M 10s\n",
      " 15300K .......... .......... .......... .......... .......... 42% 1,36M 10s\n",
      " 15350K .......... .......... .......... .......... .......... 42% 1,50M 10s\n",
      " 15400K .......... .......... .......... .......... .......... 42% 1,61M 10s\n",
      " 15450K .......... .......... .......... .......... .......... 42%  999K 10s\n",
      " 15500K .......... .......... .......... .......... .......... 42% 1012K 10s\n",
      " 15550K .......... .......... .......... .......... .......... 42% 1,61M 10s\n",
      " 15600K .......... .......... .......... .......... .......... 43%  602K 10s\n",
      " 15650K .......... .......... .......... .......... .......... 43%  596K 10s\n",
      " 15700K .......... .......... .......... .......... .......... 43%  582K 10s\n",
      " 15750K .......... .......... .......... .......... .......... 43%  712K 10s\n",
      " 15800K .......... .......... .......... .......... .......... 43%  788K 10s\n",
      " 15850K .......... .......... .......... .......... .......... 43%  955K 10s\n",
      " 15900K .......... .......... .......... .......... .......... 43% 1,02M 10s\n",
      " 15950K .......... .......... .......... .......... .......... 44% 1,12M 10s\n",
      " 16000K .......... .......... .......... .......... .......... 44%  880K 10s\n",
      " 16050K .......... .......... .......... .......... .......... 44% 1,21M 10s\n",
      " 16100K .......... .......... .......... .......... .......... 44% 1,43M 10s\n",
      " 16150K .......... .......... .......... .......... .......... 44% 1,33M 10s\n",
      " 16200K .......... .......... .......... .......... .......... 44% 1,42M 10s\n",
      " 16250K .......... .......... .......... .......... .......... 44%  868K 10s\n",
      " 16300K .......... .......... .......... .......... .......... 45% 2,71M 10s\n",
      " 16350K .......... .......... .......... .......... .......... 45% 1013K 10s\n",
      " 16400K .......... .......... .......... .......... .......... 45%  906K 10s\n",
      " 16450K .......... .......... .......... .......... .......... 45% 1,22M 10s\n",
      " 16500K .......... .......... .......... .......... .......... 45% 1,28M 10s\n",
      " 16550K .......... .......... .......... .......... .......... 45% 1,30M 10s\n",
      " 16600K .......... .......... .......... .......... .......... 45% 1,42M 10s\n",
      " 16650K .......... .......... .......... .......... .......... 45% 1,42M 10s\n",
      " 16700K .......... .......... .......... .......... .......... 46% 1,59M 10s\n",
      " 16750K .......... .......... .......... .......... .......... 46% 1,26M 10s\n",
      " 16800K .......... .......... .......... .......... .......... 46%  856K 10s\n",
      " 16850K .......... .......... .......... .......... .......... 46% 1,14M 10s\n",
      " 16900K .......... .......... .......... .......... .......... 46% 1,12M 10s\n",
      " 16950K .......... .......... .......... .......... .......... 46% 1,12M 10s\n",
      " 17000K .......... .......... .......... .......... .......... 46% 1,37M 10s\n",
      " 17050K .......... .......... .......... .......... .......... 47% 1,28M 10s\n",
      " 17100K .......... .......... .......... .......... .......... 47% 1,40M 10s\n",
      " 17150K .......... .......... .......... .......... .......... 47% 1,46M 10s\n",
      " 17200K .......... .......... .......... .......... .......... 47% 1,17M 10s\n",
      " 17250K .......... .......... .......... .......... .......... 47% 1,64M 10s\n",
      " 17300K .......... .......... .......... .......... .......... 47% 1,63M 10s\n",
      " 17350K .......... .......... .......... .......... .......... 47% 1,70M 10s\n",
      " 17400K .......... .......... .......... .......... .......... 48% 1,70M 10s\n",
      " 17450K .......... .......... .......... .......... .......... 48% 1,92M 10s\n",
      " 17500K .......... .......... .......... .......... .......... 48% 1,77M 10s\n",
      " 17550K .......... .......... .......... .......... .......... 48% 1,75M 10s\n",
      " 17600K .......... .......... .......... .......... .......... 48% 1,56M 10s\n",
      " 17650K .......... .......... .......... .......... .......... 48% 2,15M 10s\n",
      " 17700K .......... .......... .......... .......... .......... 48% 1,85M 10s\n",
      " 17750K .......... .......... .......... .......... .......... 49% 2,21M 10s\n",
      " 17800K .......... .......... .......... .......... .......... 49% 1,87M 10s\n",
      " 17850K .......... .......... .......... .......... .......... 49% 2,32M 10s\n",
      " 17900K .......... .......... .......... .......... .......... 49% 2,19M 10s\n",
      " 17950K .......... .......... .......... .......... .......... 49% 2,29M 10s\n",
      " 18000K .......... .......... .......... .......... .......... 49% 1,67M 10s\n",
      " 18050K .......... .......... .......... .......... .......... 49% 2,41M 10s\n",
      " 18100K .......... .......... .......... .......... .......... 49% 2,28M 10s\n",
      " 18150K .......... .......... .......... .......... .......... 50% 2,43M 10s\n",
      " 18200K .......... .......... .......... .......... .......... 50% 2,45M 10s\n",
      " 18250K .......... .......... .......... .......... .......... 50% 2,49M 10s\n",
      " 18300K .......... .......... .......... .......... .......... 50% 2,41M 9s\n",
      " 18350K .......... .......... .......... .......... .......... 50% 1,35M 9s\n",
      " 18400K .......... .......... .......... .......... .......... 50% 2,60M 9s\n",
      " 18450K .......... .......... .......... .......... .......... 50% 1,86M 9s\n",
      " 18500K .......... .......... .......... .......... .......... 51% 1,84M 9s\n",
      " 18550K .......... .......... .......... .......... .......... 51% 1,72M 9s\n",
      " 18600K .......... .......... .......... .......... .......... 51% 1,98M 9s\n",
      " 18650K .......... .......... .......... .......... .......... 51% 1,89M 9s\n",
      " 18700K .......... .......... .......... .......... .......... 51% 1,77M 9s\n",
      " 18750K .......... .......... .......... .......... .......... 51% 2,01M 9s\n",
      " 18800K .......... .......... .......... .......... .......... 51% 1,55M 9s\n",
      " 18850K .......... .......... .......... .......... .......... 52% 1,89M 9s\n",
      " 18900K .......... .......... .......... .......... .......... 52% 2,19M 9s\n",
      " 18950K .......... .......... .......... .......... .......... 52% 2,22M 9s\n",
      " 19000K .......... .......... .......... .......... .......... 52% 1,94M 9s\n",
      " 19050K .......... .......... .......... .......... .......... 52% 2,21M 9s\n",
      " 19100K .......... .......... .......... .......... .......... 52% 2,40M 9s\n",
      " 19150K .......... .......... .......... .......... .......... 52% 2,29M 9s\n",
      " 19200K .......... .......... .......... .......... .......... 52% 1,82M 9s\n",
      " 19250K .......... .......... .......... .......... .......... 53% 2,36M 9s\n",
      " 19300K .......... .......... .......... .......... .......... 53% 2,52M 9s\n",
      " 19350K .......... .......... .......... .......... .......... 53% 1,23M 9s\n",
      " 19400K .......... .......... .......... .......... .......... 53% 13,2M 9s\n",
      " 19450K .......... .......... .......... .......... .......... 53% 1,39M 9s\n",
      " 19500K .......... .......... .......... .......... .......... 53% 1,87M 9s\n",
      " 19550K .......... .......... .......... .......... .......... 53% 1,74M 9s\n",
      " 19600K .......... .......... .......... .......... .......... 54% 1,20M 9s\n",
      " 19650K .......... .......... .......... .......... .......... 54% 1,15M 9s\n",
      " 19700K .......... .......... .......... .......... .......... 54% 1,14M 9s\n",
      " 19750K .......... .......... .......... .......... .......... 54% 1,36M 9s\n",
      " 19800K .......... .......... .......... .......... .......... 54% 1,36M 9s\n",
      " 19850K .......... .......... .......... .......... .......... 54% 1,46M 9s\n",
      " 19900K .......... .......... .......... .......... .......... 54% 1,48M 9s\n",
      " 19950K .......... .......... .......... .......... .......... 55% 1,43M 9s\n",
      " 20000K .......... .......... .......... .......... .......... 55% 1,22M 9s\n",
      " 20050K .......... .......... .......... .......... .......... 55% 1,74M 9s\n",
      " 20100K .......... .......... .......... .......... .......... 55% 1,65M 9s\n",
      " 20150K .......... .......... .......... .......... .......... 55% 1,86M 9s\n",
      " 20200K .......... .......... .......... .......... .......... 55% 1,38M 9s\n",
      " 20250K .......... .......... .......... .......... .......... 55% 1,23M 9s\n",
      " 20300K .......... .......... .......... .......... .......... 56% 1,21M 9s\n",
      " 20350K .......... .......... .......... .......... .......... 56% 1,29M 8s\n",
      " 20400K .......... .......... .......... .......... .......... 56% 1,04M 8s\n",
      " 20450K .......... .......... .......... .......... .......... 56% 1,07M 8s\n",
      " 20500K .......... .......... .......... .......... .......... 56%  906K 8s\n",
      " 20550K .......... .......... .......... .......... .......... 56% 1,01M 8s\n",
      " 20600K .......... .......... .......... .......... .......... 56% 1,07M 8s\n",
      " 20650K .......... .......... .......... .......... .......... 56% 1,13M 8s\n",
      " 20700K .......... .......... .......... .......... .......... 57%  779K 8s\n",
      " 20750K .......... .......... .......... .......... .......... 57% 1,87M 8s\n",
      " 20800K .......... .......... .......... .......... .......... 57%  669K 8s\n",
      " 20850K .......... .......... .......... .......... .......... 57% 1,05M 8s\n",
      " 20900K .......... .......... .......... .......... .......... 57%  847K 8s\n",
      " 20950K .......... .......... .......... .......... .......... 57%  740K 8s\n",
      " 21000K .......... .......... .......... .......... .......... 57%  853K 8s\n",
      " 21050K .......... .......... .......... .......... .......... 58% 1018K 8s\n",
      " 21100K .......... .......... .......... .......... .......... 58%  816K 8s\n",
      " 21150K .......... .......... .......... .......... .......... 58%  679K 8s\n",
      " 21200K .......... .......... .......... .......... .......... 58%  453K 8s\n",
      " 21250K .......... .......... .......... .......... .......... 58%  597K 8s\n",
      " 21300K .......... .......... .......... .......... .......... 58%  717K 8s\n",
      " 21350K .......... .......... .......... .......... .......... 58%  885K 8s\n",
      " 21400K .......... .......... .......... .......... .......... 59%  903K 8s\n",
      " 21450K .......... .......... .......... .......... .......... 59% 1,00M 8s\n",
      " 21500K .......... .......... .......... .......... .......... 59% 1,13M 8s\n",
      " 21550K .......... .......... .......... .......... .......... 59% 1,13M 8s\n",
      " 21600K .......... .......... .......... .......... .......... 59%  934K 8s\n",
      " 21650K .......... .......... .......... .......... .......... 59% 1,07M 8s\n",
      " 21700K .......... .......... .......... .......... .......... 59%  673K 8s\n",
      " 21750K .......... .......... .......... .......... .......... 60% 1,18M 8s\n",
      " 21800K .......... .......... .......... .......... .......... 60%  720K 8s\n",
      " 21850K .......... .......... .......... .......... .......... 60%  772K 8s\n",
      " 21900K .......... .......... .......... .......... .......... 60%  934K 8s\n",
      " 21950K .......... .......... .......... .......... .......... 60%  984K 8s\n",
      " 22000K .......... .......... .......... .......... .......... 60%  834K 8s\n",
      " 22050K .......... .......... .......... .......... .......... 60% 1,19M 8s\n",
      " 22100K .......... .......... .......... .......... .......... 60% 1,22M 8s\n",
      " 22150K .......... .......... .......... .......... .......... 61% 1,22M 8s\n",
      " 22200K .......... .......... .......... .......... .......... 61% 1,44M 8s\n",
      " 22250K .......... .......... .......... .......... .......... 61% 1,34M 8s\n",
      " 22300K .......... .......... .......... .......... .......... 61% 1,54M 8s\n",
      " 22350K .......... .......... .......... .......... .......... 61% 1,52M 8s\n",
      " 22400K .......... .......... .......... .......... .......... 61% 1,23M 8s\n",
      " 22450K .......... .......... .......... .......... .......... 61% 1,61M 8s\n",
      " 22500K .......... .......... .......... .......... .......... 62% 1,76M 8s\n",
      " 22550K .......... .......... .......... .......... .......... 62% 1,71M 8s\n",
      " 22600K .......... .......... .......... .......... .......... 62% 1,77M 8s\n",
      " 22650K .......... .......... .......... .......... .......... 62% 1,82M 8s\n",
      " 22700K .......... .......... .......... .......... .......... 62% 1,17M 8s\n",
      " 22750K .......... .......... .......... .......... .......... 62% 1,24M 8s\n",
      " 22800K .......... .......... .......... .......... .......... 62% 1,02M 8s\n",
      " 22850K .......... .......... .......... .......... .......... 63% 1,35M 8s\n",
      " 22900K .......... .......... .......... .......... .......... 63% 1,33M 8s\n",
      " 22950K .......... .......... .......... .......... .......... 63% 1020K 8s\n",
      " 23000K .......... .......... .......... .......... .......... 63% 2,01M 8s\n",
      " 23050K .......... .......... .......... .......... .......... 63%  800K 8s\n",
      " 23100K .......... .......... .......... .......... .......... 63%  737K 8s\n",
      " 23150K .......... .......... .......... .......... .......... 63%  838K 8s\n",
      " 23200K .......... .......... .......... .......... .......... 64%  702K 8s\n",
      " 23250K .......... .......... .......... .......... .......... 64% 1,14M 8s\n",
      " 23300K .......... .......... .......... .......... .......... 64% 1,04M 8s\n",
      " 23350K .......... .......... .......... .......... .......... 64% 1,23M 8s\n",
      " 23400K .......... .......... .......... .......... .......... 64% 1,22M 8s\n",
      " 23450K .......... .......... .......... .......... .......... 64% 1,47M 8s\n",
      " 23500K .......... .......... .......... .......... .......... 64% 1,33M 8s\n",
      " 23550K .......... .......... .......... .......... .......... 64% 1,40M 8s\n",
      " 23600K .......... .......... .......... .......... .......... 65% 1,07M 8s\n",
      " 23650K .......... .......... .......... .......... .......... 65% 1,63M 8s\n",
      " 23700K .......... .......... .......... .......... .......... 65% 1,59M 7s\n",
      " 23750K .......... .......... .......... .......... .......... 65% 1,66M 7s\n",
      " 23800K .......... .......... .......... .......... .......... 65% 1,72M 7s\n",
      " 23850K .......... .......... .......... .......... .......... 65% 1,82M 7s\n",
      " 23900K .......... .......... .......... .......... .......... 65% 1,85M 7s\n",
      " 23950K .......... .......... .......... .......... .......... 66% 1,71M 7s\n",
      " 24000K .......... .......... .......... .......... .......... 66% 1,36M 7s\n",
      " 24050K .......... .......... .......... .......... .......... 66% 2,09M 7s\n",
      " 24100K .......... .......... .......... .......... .......... 66% 1,81M 7s\n",
      " 24150K .......... .......... .......... .......... .......... 66% 2,20M 7s\n",
      " 24200K .......... .......... .......... .......... .......... 66% 1,25M 7s\n",
      " 24250K .......... .......... .......... .......... .......... 66% 2,51M 7s\n",
      " 24300K .......... .......... .......... .......... .......... 67% 1,40M 7s\n",
      " 24350K .......... .......... .......... .......... .......... 67% 1,58M 7s\n",
      " 24400K .......... .......... .......... .......... .......... 67% 1,11M 7s\n",
      " 24450K .......... .......... .......... .......... .......... 67% 1,56M 7s\n",
      " 24500K .......... .......... .......... .......... .......... 67% 1,64M 7s\n",
      " 24550K .......... .......... .......... .......... .......... 67% 1,66M 7s\n",
      " 24600K .......... .......... .......... .......... .......... 67% 1,63M 7s\n",
      " 24650K .......... .......... .......... .......... .......... 67% 1,85M 7s\n",
      " 24700K .......... .......... .......... .......... .......... 68% 1,77M 7s\n",
      " 24750K .......... .......... .......... .......... .......... 68% 1,91M 7s\n",
      " 24800K .......... .......... .......... .......... .......... 68% 1,39M 7s\n",
      " 24850K .......... .......... .......... .......... .......... 68% 1,85M 7s\n",
      " 24900K .......... .......... .......... .......... .......... 68% 2,07M 7s\n",
      " 24950K .......... .......... .......... .......... .......... 68% 1,54M 7s\n",
      " 25000K .......... .......... .......... .......... .......... 68% 1,82M 7s\n",
      " 25050K .......... .......... .......... .......... .......... 69% 1,38M 7s\n",
      " 25100K .......... .......... .......... .......... .......... 69% 1,48M 7s\n",
      " 25150K .......... .......... .......... .......... .......... 69% 1,52M 7s\n",
      " 25200K .......... .......... .......... .......... .......... 69% 1,08M 7s\n",
      " 25250K .......... .......... .......... .......... .......... 69% 1,71M 7s\n",
      " 25300K .......... .......... .......... .......... .......... 69%  939K 7s\n",
      " 25350K .......... .......... .......... .......... .......... 69% 3,36M 7s\n",
      " 25400K .......... .......... .......... .......... .......... 70% 1,23M 6s\n",
      " 25450K .......... .......... .......... .......... .......... 70% 1,22M 6s\n",
      " 25500K .......... .......... .......... .......... .......... 70% 1,28M 6s\n",
      " 25550K .......... .......... .......... .......... .......... 70% 1,31M 6s\n",
      " 25600K .......... .......... .......... .......... .......... 70% 1,08M 6s\n",
      " 25650K .......... .......... .......... .......... .......... 70% 1,45M 6s\n",
      " 25700K .......... .......... .......... .......... .......... 70%  961K 6s\n",
      " 25750K .......... .......... .......... .......... .......... 71% 1,88M 6s\n",
      " 25800K .......... .......... .......... .......... .......... 71% 1,02M 6s\n",
      " 25850K .......... .......... .......... .......... .......... 71%  713K 6s\n",
      " 25900K .......... .......... .......... .......... .......... 71%  537K 6s\n",
      " 25950K .......... .......... .......... .......... .......... 71%  622K 6s\n",
      " 26000K .......... .......... .......... .......... .......... 71%  553K 6s\n",
      " 26050K .......... .......... .......... .......... .......... 71%  900K 6s\n",
      " 26100K .......... .......... .......... .......... .......... 71%  994K 6s\n",
      " 26150K .......... .......... .......... .......... .......... 72% 1,11M 6s\n",
      " 26200K .......... .......... .......... .......... .......... 72% 1,10M 6s\n",
      " 26250K .......... .......... .......... .......... .......... 72% 1,17M 6s\n",
      " 26300K .......... .......... .......... .......... .......... 72% 1,15M 6s\n",
      " 26350K .......... .......... .......... .......... .......... 72%  819K 6s\n",
      " 26400K .......... .......... .......... .......... .......... 72% 1,20M 6s\n",
      " 26450K .......... .......... .......... .......... .......... 72%  984K 6s\n",
      " 26500K .......... .......... .......... .......... .......... 73%  848K 6s\n",
      " 26550K .......... .......... .......... .......... .......... 73%  884K 6s\n",
      " 26600K .......... .......... .......... .......... .......... 73%  829K 6s\n",
      " 26650K .......... .......... .......... .......... .......... 73%  916K 6s\n",
      " 26700K .......... .......... .......... .......... .......... 73%  976K 6s\n",
      " 26750K .......... .......... .......... .......... .......... 73% 1,13M 6s\n",
      " 26800K .......... .......... .......... .......... .......... 73%  829K 6s\n",
      " 26850K .......... .......... .......... .......... .......... 74% 1,26M 6s\n",
      " 26900K .......... .......... .......... .......... .......... 74% 1,18M 6s\n",
      " 26950K .......... .......... .......... .......... .......... 74% 1,28M 6s\n",
      " 27000K .......... .......... .......... .......... .......... 74% 1,31M 6s\n",
      " 27050K .......... .......... .......... .......... .......... 74% 1,55M 6s\n",
      " 27100K .......... .......... .......... .......... .......... 74% 1,50M 6s\n",
      " 27150K .......... .......... .......... .......... .......... 74% 1,61M 6s\n",
      " 27200K .......... .......... .......... .......... .......... 75%  916K 6s\n",
      " 27250K .......... .......... .......... .......... .......... 75% 1,76M 6s\n",
      " 27300K .......... .......... .......... .......... .......... 75% 1,29M 6s\n",
      " 27350K .......... .......... .......... .......... .......... 75% 1,22M 6s\n",
      " 27400K .......... .......... .......... .......... .......... 75% 1,30M 6s\n",
      " 27450K .......... .......... .......... .......... .......... 75% 1,29M 5s\n",
      " 27500K .......... .......... .......... .......... .......... 75% 1,47M 5s\n",
      " 27550K .......... .......... .......... .......... .......... 75% 1,38M 5s\n",
      " 27600K .......... .......... .......... .......... .......... 76% 1,12M 5s\n",
      " 27650K .......... .......... .......... .......... .......... 76% 1,54M 5s\n",
      " 27700K .......... .......... .......... .......... .......... 76% 1,66M 5s\n",
      " 27750K .......... .......... .......... .......... .......... 76% 1,21M 5s\n",
      " 27800K .......... .......... .......... .......... .......... 76% 1,46M 5s\n",
      " 27850K .......... .......... .......... .......... .......... 76% 1,18M 5s\n",
      " 27900K .......... .......... .......... .......... .......... 76%  909K 5s\n",
      " 27950K .......... .......... .......... .......... .......... 77%  909K 5s\n",
      " 28000K .......... .......... .......... .......... .......... 77%  541K 5s\n",
      " 28050K .......... .......... .......... .......... .......... 77%  486K 5s\n",
      " 28100K .......... .......... .......... .......... .......... 77%  554K 5s\n",
      " 28150K .......... .......... .......... .......... .......... 77%  735K 5s\n",
      " 28200K .......... .......... .......... .......... .......... 77%  853K 5s\n",
      " 28250K .......... .......... .......... .......... .......... 77%  964K 5s\n",
      " 28300K .......... .......... .......... .......... .......... 78%  839K 5s\n",
      " 28350K .......... .......... .......... .......... .......... 78%  642K 5s\n",
      " 28400K .......... .......... .......... .......... .......... 78%  658K 5s\n",
      " 28450K .......... .......... .......... .......... .......... 78%  885K 5s\n",
      " 28500K .......... .......... .......... .......... .......... 78%  959K 5s\n",
      " 28550K .......... .......... .......... .......... .......... 78% 1,11M 5s\n",
      " 28600K .......... .......... .......... .......... .......... 78% 1,11M 5s\n",
      " 28650K .......... .......... .......... .......... .......... 79% 1,26M 5s\n",
      " 28700K .......... .......... .......... .......... .......... 79% 1,27M 5s\n",
      " 28750K .......... .......... .......... .......... .......... 79% 1,45M 5s\n",
      " 28800K .......... .......... .......... .......... .......... 79% 1010K 5s\n",
      " 28850K .......... .......... .......... .......... .......... 79% 1,61M 5s\n",
      " 28900K .......... .......... .......... .......... .......... 79% 1,37M 5s\n",
      " 28950K .......... .......... .......... .......... .......... 79% 1,66M 5s\n",
      " 29000K .......... .......... .......... .......... .......... 79% 1,59M 5s\n",
      " 29050K .......... .......... .......... .......... .......... 80% 1,70M 5s\n",
      " 29100K .......... .......... .......... .......... .......... 80% 1,84M 5s\n",
      " 29150K .......... .......... .......... .......... .......... 80% 1,72M 5s\n",
      " 29200K .......... .......... .......... .......... .......... 80% 1,36M 5s\n",
      " 29250K .......... .......... .......... .......... .......... 80% 2,02M 5s\n",
      " 29300K .......... .......... .......... .......... .......... 80% 1,83M 4s\n",
      " 29350K .......... .......... .......... .......... .......... 80% 2,04M 4s\n",
      " 29400K .......... .......... .......... .......... .......... 81% 1,59M 4s\n",
      " 29450K .......... .......... .......... .......... .......... 81% 1,75M 4s\n",
      " 29500K .......... .......... .......... .......... .......... 81% 1,40M 4s\n",
      " 29550K .......... .......... .......... .......... .......... 81% 1,55M 4s\n",
      " 29600K .......... .......... .......... .......... .......... 81% 1,10M 4s\n",
      " 29650K .......... .......... .......... .......... .......... 81% 1,58M 4s\n",
      " 29700K .......... .......... .......... .......... .......... 81% 1,63M 4s\n",
      " 29750K .......... .......... .......... .......... .......... 82% 1,70M 4s\n",
      " 29800K .......... .......... .......... .......... .......... 82% 1,69M 4s\n",
      " 29850K .......... .......... .......... .......... .......... 82% 1,71M 4s\n",
      " 29900K .......... .......... .......... .......... .......... 82% 1,74M 4s\n",
      " 29950K .......... .......... .......... .......... .......... 82% 2,05M 4s\n",
      " 30000K .......... .......... .......... .......... .......... 82% 1,48M 4s\n",
      " 30050K .......... .......... .......... .......... .......... 82% 1,90M 4s\n",
      " 30100K .......... .......... .......... .......... .......... 83% 2,13M 4s\n",
      " 30150K .......... .......... .......... .......... .......... 83% 1,90M 4s\n",
      " 30200K .......... .......... .......... .......... .......... 83% 2,22M 4s\n",
      " 30250K .......... .......... .......... .......... .......... 83% 2,12M 4s\n",
      " 30300K .......... .......... .......... .......... .......... 83% 2,11M 4s\n",
      " 30350K .......... .......... .......... .......... .......... 83% 1,18M 4s\n",
      " 30400K .......... .......... .......... .......... .......... 83% 2,44M 4s\n",
      " 30450K .......... .......... .......... .......... .......... 83% 1,43M 4s\n",
      " 30500K .......... .......... .......... .......... .......... 84% 1,40M 4s\n",
      " 30550K .......... .......... .......... .......... .......... 84% 1,56M 4s\n",
      " 30600K .......... .......... .......... .......... .......... 84% 1,62M 4s\n",
      " 30650K .......... .......... .......... .......... .......... 84% 1,15M 4s\n",
      " 30700K .......... .......... .......... .......... .......... 84% 1,22M 4s\n",
      " 30750K .......... .......... .......... .......... .......... 84% 1,18M 4s\n",
      " 30800K .......... .......... .......... .......... .......... 84% 1024K 4s\n",
      " 30850K .......... .......... .......... .......... .......... 85% 1,31M 3s\n",
      " 30900K .......... .......... .......... .......... .......... 85% 1,24M 3s\n",
      " 30950K .......... .......... .......... .......... .......... 85%  947K 3s\n",
      " 31000K .......... .......... .......... .......... .......... 85%  969K 3s\n",
      " 31050K .......... .......... .......... .......... .......... 85% 1,09M 3s\n",
      " 31100K .......... .......... .......... .......... .......... 85% 1,09M 3s\n",
      " 31150K .......... .......... .......... .......... .......... 85% 1,09M 3s\n",
      " 31200K .......... .......... .......... .......... .......... 86% 1,01M 3s\n",
      " 31250K .......... .......... .......... .......... .......... 86% 1,36M 3s\n",
      " 31300K .......... .......... .......... .......... .......... 86% 1,32M 3s\n",
      " 31350K .......... .......... .......... .......... .......... 86% 1,58M 3s\n",
      " 31400K .......... .......... .......... .......... .......... 86% 1,54M 3s\n",
      " 31450K .......... .......... .......... .......... .......... 86% 1,60M 3s\n",
      " 31500K .......... .......... .......... .......... .......... 86% 1,64M 3s\n",
      " 31550K .......... .......... .......... .......... .......... 86% 1,65M 3s\n",
      " 31600K .......... .......... .......... .......... .......... 87% 1,25M 3s\n",
      " 31650K .......... .......... .......... .......... .......... 87% 1,91M 3s\n",
      " 31700K .......... .......... .......... .......... .......... 87% 1,83M 3s\n",
      " 31750K .......... .......... .......... .......... .......... 87% 1,78M 3s\n",
      " 31800K .......... .......... .......... .......... .......... 87% 1,76M 3s\n",
      " 31850K .......... .......... .......... .......... .......... 87% 2,08M 3s\n",
      " 31900K .......... .......... .......... .......... .......... 87% 1,88M 3s\n",
      " 31950K .......... .......... .......... .......... .......... 88% 2,22M 3s\n",
      " 32000K .......... .......... .......... .......... .......... 88% 1,54M 3s\n",
      " 32050K .......... .......... .......... .......... .......... 88% 2,28M 3s\n",
      " 32100K .......... .......... .......... .......... .......... 88% 2,24M 3s\n",
      " 32150K .......... .......... .......... .......... .......... 88% 2,01M 3s\n",
      " 32200K .......... .......... .......... .......... .......... 88% 2,39M 3s\n",
      " 32250K .......... .......... .......... .......... .......... 88% 2,30M 3s\n",
      " 32300K .......... .......... .......... .......... .......... 89% 2,08M 3s\n",
      " 32350K .......... .......... .......... .......... .......... 89% 2,40M 3s\n",
      " 32400K .......... .......... .......... .......... .......... 89% 1,69M 2s\n",
      " 32450K .......... .......... .......... .......... .......... 89% 1,08M 2s\n",
      " 32500K .......... .......... .......... .......... .......... 89%  506M 2s\n",
      " 32550K .......... .......... .......... .......... .......... 89% 2,17M 2s\n",
      " 32600K .......... .......... .......... .......... .......... 89% 1,55M 2s\n",
      " 32650K .......... .......... .......... .......... .......... 90% 1,82M 2s\n",
      " 32700K .......... .......... .......... .......... .......... 90% 1,19M 2s\n",
      " 32750K .......... .......... .......... .......... .......... 90% 1,58M 2s\n",
      " 32800K .......... .......... .......... .......... .......... 90%  970K 2s\n",
      " 32850K .......... .......... .......... .......... .......... 90% 1,24M 2s\n",
      " 32900K .......... .......... .......... .......... .......... 90%  815K 2s\n",
      " 32950K .......... .......... .......... .......... .......... 90% 1,52M 2s\n",
      " 33000K .......... .......... .......... .......... .......... 90%  923K 2s\n",
      " 33050K .......... .......... .......... .......... .......... 91% 1,04M 2s\n",
      " 33100K .......... .......... .......... .......... .......... 91% 1,14M 2s\n",
      " 33150K .......... .......... .......... .......... .......... 91% 1,07M 2s\n",
      " 33200K .......... .......... .......... .......... .......... 91%  919K 2s\n",
      " 33250K .......... .......... .......... .......... .......... 91% 1,44M 2s\n",
      " 33300K .......... .......... .......... .......... .......... 91% 1,33M 2s\n",
      " 33350K .......... .......... .......... .......... .......... 91% 1,48M 2s\n",
      " 33400K .......... .......... .......... .......... .......... 92% 1,60M 2s\n",
      " 33450K .......... .......... .......... .......... .......... 92% 1,39M 2s\n",
      " 33500K .......... .......... .......... .......... .......... 92% 1,67M 2s\n",
      " 33550K .......... .......... .......... .......... .......... 92% 1,70M 2s\n",
      " 33600K .......... .......... .......... .......... .......... 92% 1,25M 2s\n",
      " 33650K .......... .......... .......... .......... .......... 92% 1,84M 2s\n",
      " 33700K .......... .......... .......... .......... .......... 92% 1,68M 2s\n",
      " 33750K .......... .......... .......... .......... .......... 93% 1,87M 2s\n",
      " 33800K .......... .......... .......... .......... .......... 93% 1,81M 2s\n",
      " 33850K .......... .......... .......... .......... .......... 93% 1,27M 2s\n",
      " 33900K .......... .......... .......... .......... .......... 93% 1,32M 2s\n",
      " 33950K .......... .......... .......... .......... .......... 93% 1,39M 1s\n",
      " 34000K .......... .......... .......... .......... .......... 93% 1,13M 1s\n",
      " 34050K .......... .......... .......... .......... .......... 93% 1,59M 1s\n",
      " 34100K .......... .......... .......... .......... .......... 94% 1,62M 1s\n",
      " 34150K .......... .......... .......... .......... .......... 94% 1,55M 1s\n",
      " 34200K .......... .......... .......... .......... .......... 94% 1,69M 1s\n",
      " 34250K .......... .......... .......... .......... .......... 94% 1,85M 1s\n",
      " 34300K .......... .......... .......... .......... .......... 94% 1,01M 1s\n",
      " 34350K .......... .......... .......... .......... .......... 94% 2,96M 1s\n",
      " 34400K .......... .......... .......... .......... .......... 94%  871K 1s\n",
      " 34450K .......... .......... .......... .......... .......... 94% 1,37M 1s\n",
      " 34500K .......... .......... .......... .......... .......... 95% 1,28M 1s\n",
      " 34550K .......... .......... .......... .......... .......... 95% 1,50M 1s\n",
      " 34600K .......... .......... .......... .......... .......... 95% 1,26M 1s\n",
      " 34650K .......... .......... .......... .......... .......... 95% 1,57M 1s\n",
      " 34700K .......... .......... .......... .......... .......... 95% 1,42M 1s\n",
      " 34750K .......... .......... .......... .......... .......... 95% 1,64M 1s\n",
      " 34800K .......... .......... .......... .......... .......... 95% 1,22M 1s\n",
      " 34850K .......... .......... .......... .......... .......... 96% 1,91M 1s\n",
      " 34900K .......... .......... .......... .......... .......... 96% 1,73M 1s\n",
      " 34950K .......... .......... .......... .......... .......... 96% 1,06M 1s\n",
      " 35000K .......... .......... .......... .......... .......... 96% 3,01M 1s\n",
      " 35050K .......... .......... .......... .......... .......... 96% 1,30M 1s\n",
      " 35100K .......... .......... .......... .......... .......... 96% 1,28M 1s\n",
      " 35150K .......... .......... .......... .......... .......... 96% 1,50M 1s\n",
      " 35200K .......... .......... .......... .......... .......... 97%  655K 1s\n",
      " 35250K .......... .......... .......... .......... .......... 97% 3,43M 1s\n",
      " 35300K .......... .......... .......... .......... .......... 97% 1,04M 1s\n",
      " 35350K .......... .......... .......... .......... .......... 97% 1,12M 1s\n",
      " 35400K .......... .......... .......... .......... .......... 97% 1,18M 1s\n",
      " 35450K .......... .......... .......... .......... .......... 97% 1,26M 1s\n",
      " 35500K .......... .......... .......... .......... .......... 97% 1,28M 1s\n",
      " 35550K .......... .......... .......... .......... .......... 98% 1,31M 0s\n",
      " 35600K .......... .......... .......... .......... .......... 98%  929K 0s\n",
      " 35650K .......... .......... .......... .......... .......... 98% 1,01M 0s\n",
      " 35700K .......... .......... .......... .......... .......... 98% 1,15M 0s\n",
      " 35750K .......... .......... .......... .......... .......... 98% 1,08M 0s\n",
      " 35800K .......... .......... .......... .......... .......... 98% 1,29M 0s\n",
      " 35850K .......... .......... .......... .......... .......... 98% 1,22M 0s\n",
      " 35900K .......... .......... .......... .......... .......... 98% 1,36M 0s\n",
      " 35950K .......... .......... .......... .......... .......... 99% 1,34M 0s\n",
      " 36000K .......... .......... .......... .......... .......... 99% 1,11M 0s\n",
      " 36050K .......... .......... .......... .......... .......... 99% 1,42M 0s\n",
      " 36100K .......... .......... .......... .......... .......... 99% 1,59M 0s\n",
      " 36150K .......... .......... .......... .......... .......... 99% 1,61M 0s\n",
      " 36200K .......... .......... .......... .......... .......... 99% 1,87M 0s\n",
      " 36250K .......... .......... .......... .......... .......... 99% 1,71M 0s\n",
      " 36300K .......... .......... .....                           100% 2,39M=24s\n",
      "\n",
      "2023-01-16 14:56:06 (1,50 MB/s) - ‘cmake-3.14.4-Linux-x86_64.sh’ saved [37196929/37196929]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMake Installer Version: 3.14.4, Copyright (c) Kitware\n",
      "This is a self-extracting archive.\n",
      "The archive will be extracted to: /usr/local\n",
      "\n",
      "Using target directory: /usr/local\n",
      "Extracting, please wait...\n",
      "\n",
      "Unpacking finished successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 11: python3.6: not found\n",
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 14: python3.6: not found\n",
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 15: python3.6: not found\n",
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 18: python3.6: not found\n",
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 19: python3.6: not found\n",
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 20: python3.6: not found\n",
      "/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/setup_env_desktop.sh: 23: python3.6: not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
    "    os.environ[\"bash_script\"] = \"setup_env.sh\"\n",
    "else:\n",
    "    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n",
    "\n",
    "!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
    "\n",
    "os.system(f'echo {password} | sudo -S sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"PYTHONPATH\",\"\") == \"\":\n",
    "    os.environ[\"PYTHONPATH\"] = \"\"\n",
    "os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n",
    "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
    "    os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\"\n",
    "else:\n",
    "    os.environ[\"PYTHONPATH\"]+=\":/home_duplicate/rarunachalam/miniconda3/envs/tf_py_36/lib/python3.6/site-packages/third_party/nvml\" # FIX MINICONDA PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SPECs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config {\n",
      "  arch: \"resnet\",\n",
      "  n_layers: 18\n",
      "  # Setting these parameters to true to match the template downloaded from NGC.\n",
      "  use_batch_norm: true\n",
      "  all_projections: true\n",
      "  freeze_blocks: 0\n",
      "  freeze_blocks: 1\n",
      "  input_image_size: \"3,100,100\"\n",
      "}\n",
      "train_config {\n",
      "  train_dataset_path: \"/home/fkurt/Murat/TAO/GenderClassification/data//split/train\"\n",
      "  val_dataset_path: \"/home/fkurt/Murat/TAO/GenderClassification/data//split/val\"\n",
      "  pretrained_model_path: \"/home/fkurt/Murat/TAO/results/gender_classification//pretrained_resnet18/pretrained_classification_vresnet18/resnet_18.hdf5\"\n",
      "  optimizer {\n",
      "    sgd {\n",
      "    lr: 0.01\n",
      "    decay: 0.0\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "  }\n",
      "}\n",
      "  batch_size_per_gpu: 64\n",
      "  n_epochs: 10\n",
      "  n_workers: 16\n",
      "  preprocess_mode: \"caffe\"\n",
      "  enable_random_crop: True\n",
      "  enable_center_crop: True\n",
      "  label_smoothing: 0.0\n",
      "  mixup_alpha: 0.1\n",
      "  # regularizer\n",
      "  reg_config {\n",
      "    type: \"L2\"\n",
      "    scope: \"Conv2D,Dense\"\n",
      "    weight_decay: 0.00005\n",
      "  }\n",
      "\n",
      "  # learning_rate\n",
      "  lr_config {\n",
      "    step {\n",
      "      learning_rate: 0.006\n",
      "      step_size: 10\n",
      "      gamma: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  eval_dataset_path: \"/home/fkurt/Murat/TAO/GenderClassification/data//split/test\"\n",
      "  model_path: \"/home/fkurt/Murat/TAO/results/gender_classification//output/weights/resnet_010.tlt\"\n",
      "  top_k: 3\n",
      "  batch_size: 256\n",
      "  n_workers: 8\n",
      "  enable_center_crop: True\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/classification_spec.cfg\n",
    "!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/classification_spec.cfg\n",
    "!cat $SPECS_DIR/classification_spec.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "WARNING:tlt.components.docker_handler.docker_handler:\n",
      "Docker will run the commands as root. If you would like to retain your\n",
      "local host permissions, please add the \"user\":\"UID:GID\" in the\n",
      "DockerOptions portion of the \"/home/fkurt/.tao_mounts.json\" file. You can obtain your\n",
      "users UID and GID by using the \"id -u\" and \"id -g\" commands on the\n",
      "terminal.\n",
      "Using TensorFlow backend.\n",
      "2023-01-16 12:05:37.751365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "2023-01-16 12:05:39.925180: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2899885000 Hz\n",
      "2023-01-16 12:05:39.925678: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f62a50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 12:05:39.925692: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-16 12:05:39.927034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-16 12:05:39.994060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:05:39.994270: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49218e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 12:05:39.994285: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2023-01-16 12:05:39.994427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:05:39.994500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\n",
      "pciBusID: 0000:01:00.0\n",
      "2023-01-16 12:05:39.994523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 12:05:40.007648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-16 12:05:40.009502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-16 12:05:40.009707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-16 12:05:40.010140: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-01-16 12:05:40.010656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-16 12:05:40.010755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-16 12:05:40.010829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:05:40.010943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:05:40.011003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-01-16 12:05:40.011021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 12:05:40.225208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-16 12:05:40.225236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-01-16 12:05:40.225243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-01-16 12:05:40.225466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:05:40.225712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:05:40.225900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4414 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n",
      "2023-01-16 12:05:42,941 [INFO] __main__: Loading experiment spec at /home/fkurt/Murat/TAO/nvidia-tao/tensorflow/classification/specs/classification_spec.cfg.\n",
      "Traceback (most recent call last):\n",
      "  File \"</usr/local/lib/python3.6/dist-packages/iva/makenet/scripts/train.py>\", line 3, in <module>\n",
      "  File \"<frozen iva.makenet.scripts.train>\", line 666, in <module>\n",
      "  File \"<frozen iva.common.utils>\", line 707, in return_func\n",
      "  File \"<frozen iva.common.utils>\", line 695, in return_func\n",
      "  File \"<frozen iva.makenet.scripts.train>\", line 662, in main\n",
      "  File \"<frozen iva.makenet.scripts.train>\", line 648, in main\n",
      "  File \"<frozen iva.makenet.scripts.train>\", line 378, in run_experiment\n",
      "  File \"<frozen iva.makenet.spec_handling.spec_loader>\", line 114, in load_experiment_spec\n",
      "  File \"<frozen iva.makenet.spec_handling.spec_loader>\", line 90, in load_proto\n",
      "  File \"<frozen iva.makenet.spec_handling.spec_loader>\", line 74, in _load_from_file\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/fkurt/Murat/TAO/nvidia-tao/tensorflow/classification/specs/classification_spec.cfg'\n",
      "Telemetry data couldn't be sent, but the command ran successfully.\n",
      "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
      "Execution status: FAIL\n"
     ]
    }
   ],
   "source": [
    "!tao classification_tf1 train -e $SPECS_DIR/classification_spec.cfg -r $EXPERIMENT_DIR/output -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2023-01-16 14:12:04.709120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "2023-01-16 14:12:06.702871: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2899885000 Hz\n",
      "2023-01-16 14:12:06.703386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a7c950 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 14:12:06.703401: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-16 14:12:06.704717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-16 14:12:06.747862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:12:06.748023: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x494c020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 14:12:06.748044: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2023-01-16 14:12:06.748218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:12:06.748293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\n",
      "pciBusID: 0000:01:00.0\n",
      "2023-01-16 14:12:06.748316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 14:12:06.760798: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-16 14:12:06.762707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-16 14:12:06.762968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-16 14:12:06.763392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-01-16 14:12:06.763904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-16 14:12:06.764006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-16 14:12:06.764086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:12:06.764235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:12:06.764297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-01-16 14:12:06.764318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 14:12:06.972235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-16 14:12:06.972263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-01-16 14:12:06.972268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-01-16 14:12:06.972404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:12:06.972530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:12:06.972610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4506 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "INFO: Loading experiment spec at /root/TAO/nvidia-tao/tensorflow/classification/specs/classification_spec.cfg.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/third_party/keras/tensorflow_backend.py:187: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/third_party/keras/tensorflow_backend.py:187: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 100, 100)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 50, 50)   9408        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 50, 50)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 50, 50)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (Conv2D)        (None, 64, 25, 25)   36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 64, 25, 25)   256         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (Activation)    (None, 64, 25, 25)   0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (Conv2D)        (None, 64, 25, 25)   36864       block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Conv2D) (None, 64, 25, 25)   4096        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 25, 25)   256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 25, 25)   256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 25, 25)   0           block_1a_bn_2[0][0]              \n",
      "                                                                 block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (Activation)      (None, 64, 25, 25)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (Conv2D)        (None, 64, 25, 25)   36864       block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 25, 25)   256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (Activation)    (None, 64, 25, 25)   0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (Conv2D)        (None, 64, 25, 25)   36864       block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_shortcut (Conv2D) (None, 64, 25, 25)   4096        block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 25, 25)   256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_shortcut (BatchNorm (None, 64, 25, 25)   256         block_1b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 25, 25)   0           block_1b_bn_2[0][0]              \n",
      "                                                                 block_1b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (Activation)      (None, 64, 25, 25)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (Conv2D)        (None, 128, 13, 13)  73728       block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 128, 13, 13)  512         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (Activation)    (None, 128, 13, 13)  0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (Conv2D)        (None, 128, 13, 13)  147456      block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Conv2D) (None, 128, 13, 13)  8192        block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 128, 13, 13)  512         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 128, 13, 13)  512         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, 13, 13)  0           block_2a_bn_2[0][0]              \n",
      "                                                                 block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (Activation)      (None, 128, 13, 13)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (Conv2D)        (None, 128, 13, 13)  147456      block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 128, 13, 13)  512         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (Activation)    (None, 128, 13, 13)  0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (Conv2D)        (None, 128, 13, 13)  147456      block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_shortcut (Conv2D) (None, 128, 13, 13)  16384       block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 128, 13, 13)  512         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_shortcut (BatchNorm (None, 128, 13, 13)  512         block_2b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 128, 13, 13)  0           block_2b_bn_2[0][0]              \n",
      "                                                                 block_2b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (Activation)      (None, 128, 13, 13)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (Conv2D)        (None, 256, 7, 7)    294912      block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 256, 7, 7)    1024        block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (Activation)    (None, 256, 7, 7)    0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (Conv2D)        (None, 256, 7, 7)    589824      block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Conv2D) (None, 256, 7, 7)    32768       block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 256, 7, 7)    1024        block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 256, 7, 7)    1024        block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 7, 7)    0           block_3a_bn_2[0][0]              \n",
      "                                                                 block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (Activation)      (None, 256, 7, 7)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (Conv2D)        (None, 256, 7, 7)    589824      block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 256, 7, 7)    1024        block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (Activation)    (None, 256, 7, 7)    0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (Conv2D)        (None, 256, 7, 7)    589824      block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_shortcut (Conv2D) (None, 256, 7, 7)    65536       block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 256, 7, 7)    1024        block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_shortcut (BatchNorm (None, 256, 7, 7)    1024        block_3b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256, 7, 7)    0           block_3b_bn_2[0][0]              \n",
      "                                                                 block_3b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (Activation)      (None, 256, 7, 7)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (Conv2D)        (None, 512, 7, 7)    1179648     block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 512, 7, 7)    2048        block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (Activation)    (None, 512, 7, 7)    0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (Conv2D)        (None, 512, 7, 7)    2359296     block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Conv2D) (None, 512, 7, 7)    131072      block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 512, 7, 7)    2048        block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 512, 7, 7)    2048        block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 7, 7)    0           block_4a_bn_2[0][0]              \n",
      "                                                                 block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (Activation)      (None, 512, 7, 7)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (Conv2D)        (None, 512, 7, 7)    2359296     block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 512, 7, 7)    2048        block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (Activation)    (None, 512, 7, 7)    0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (Conv2D)        (None, 512, 7, 7)    2359296     block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_shortcut (Conv2D) (None, 512, 7, 7)    262144      block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 512, 7, 7)    2048        block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_shortcut (BatchNorm (None, 512, 7, 7)    2048        block_4b_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 512, 7, 7)    0           block_4b_bn_2[0][0]              \n",
      "                                                                 block_4b_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (Activation)      (None, 512, 7, 7)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 512, 1, 1)    0           block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 2)            1026        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,543,490\n",
      "Trainable params: 11,366,786\n",
      "Non-trainable params: 176,704\n",
      "__________________________________________________________________________________________________\n",
      "Found 5867 images belonging to 2 classes.\n",
      "INFO: Processing dataset (evaluation): /root/TAO/GenderClassification/data//split/test\n",
      "Evaluation Loss: 0.3367924957588704\n",
      "Evaluation Top K accuracy: 1.0\n",
      "Found 5867 images belonging to 2 classes.\n",
      "INFO: Calculating per-class P/R and confusion matrix. It may take a while...\n",
      "Confusion Matrix\n",
      "[[2752  157]\n",
      " [ 104 2854]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.96      0.95      0.95      2909\n",
      "        male       0.95      0.96      0.96      2958\n",
      "\n",
      "    accuracy                           0.96      5867\n",
      "   macro avg       0.96      0.96      0.96      5867\n",
      "weighted avg       0.96      0.96      0.96      5867\n",
      "\n",
      "Telemetry data couldn't be sent, but the command ran successfully.\n",
      "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
      "Execution status: PASS\n"
     ]
    }
   ],
   "source": [
    "!classification_tf1 evaluate -e $SPECS_DIR/classification_spec.cfg -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: EPOCH=005\n",
      "Using TensorFlow backend.\n",
      "2023-01-16 14:14:57.588565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "2023-01-16 14:14:59.563192: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2899885000 Hz\n",
      "2023-01-16 14:14:59.563657: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x643c360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 14:14:59.563673: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-16 14:14:59.564941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-16 14:14:59.610128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:14:59.610369: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x630a460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 14:14:59.610386: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2023-01-16 14:14:59.610542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:14:59.610617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\n",
      "pciBusID: 0000:01:00.0\n",
      "2023-01-16 14:14:59.610642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 14:14:59.623386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-16 14:14:59.625344: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-16 14:14:59.625637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-16 14:14:59.626084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-01-16 14:14:59.626601: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-16 14:14:59.626716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-16 14:14:59.626798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:14:59.626922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:14:59.626985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-01-16 14:14:59.627005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 14:14:59.837934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-16 14:14:59.837964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-01-16 14:14:59.837970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-01-16 14:14:59.838116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:14:59.838243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:14:59.838322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4500 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "2023-01-16 14:15:02,156 [INFO] iva.common.logging.logging: Log file already exists at /root/TAO/results/output/logs/status.json\n",
      "Invalid decryption. Unable to open file (file signature not found). The key used to load the model is incorrect.\n",
      "Telemetry data couldn't be sent, but the command ran successfully.\n",
      "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
      "Execution status: FAIL\n"
     ]
    }
   ],
   "source": [
    "# Defining the checkpoint epoch number of the model to be used for the pruning.\n",
    "# This should be lesser than the number of epochs training has been run for, in case training was interrupted earlier.\n",
    "# By default, the default final model is at epoch 010.\n",
    "%env EPOCH=005\n",
    "!mkdir -p $EXPERIMENT_DIR/output/resnet_pruned\n",
    "!classification_tf1 prune -m /root/TAO/results/output/weights/resnet_$EPOCH.tlt \\\n",
    "           -o /root/TAO/results/output/resnet_pruned/resnet18_nopool_bn_pruned.tlt \\\n",
    "           -eq union \\\n",
    "           -pth 0.6 \\\n",
    "           -k nvidia-tao \\\n",
    "           --results_dir /root/TAO/results/output/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model:\n",
      "------------\n",
      "total 39364\n",
      "-rwxrwxrwx 1 root root 40308176 Jan 16 13:58 resnet18_nopool_bn_pruned.tlt\n"
     ]
    }
   ],
   "source": [
    "print('Pruned model:')\n",
    "print('------------')\n",
    "!ls -rlt $EXPERIMENT_DIR/output/resnet_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrain Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config {\n",
      "  arch: \"resnet\",\n",
      "  n_layers: 18\n",
      "  use_batch_norm: true\n",
      "  all_projections: true\n",
      "  input_image_size: \"3,100,100\"\n",
      "}\n",
      "train_config {\n",
      "  train_dataset_path: \"/root/TAO/GenderClassification/data//split/train\"\n",
      "  val_dataset_path: \"/root/TAO/GenderClassification/data//split/val\"\n",
      "  pretrained_model_path: \"/root/TAO/results/output/resnet_pruned/resnet18_nopool_bn_pruned.tlt\"\n",
      "  optimizer {\n",
      "    sgd {\n",
      "    lr: 0.01\n",
      "    decay: 0.0\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "  }\n",
      "}\n",
      "  batch_size_per_gpu: 8\n",
      "  n_epochs: 10\n",
      "  n_workers: 16\n",
      "  preprocess_mode: \"caffe\"\n",
      "  enable_random_crop: True\n",
      "  enable_center_crop: True\n",
      "  label_smoothing: 0.0\n",
      "  mixup_alpha: 0.1\n",
      "  # regularizer\n",
      "  reg_config {\n",
      "    type: \"L2\"\n",
      "    scope: \"Conv2D,Dense\"\n",
      "    weight_decay: 0.00005\n",
      "  }\n",
      "\n",
      "  # learning_rate\n",
      "  lr_config {\n",
      "    step {\n",
      "      learning_rate: 0.006\n",
      "      step_size: 10\n",
      "      gamma: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  eval_dataset_path: \"/root/TAO/GenderClassification/data//split/test\"\n",
      "  model_path: \"/root/TAO/results/output/resnet_pruned/resnet18_nopool_bn_pruned.tlt\"\n",
      "  top_k: 3\n",
      "  batch_size: 256\n",
      "  n_workers: 8\n",
      "  enable_center_crop: True\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/classification_retrain_spec.cfg\n",
    "!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/classification_retrain_spec.cfg\n",
    "!cat $SPECS_DIR/classification_retrain_spec.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2023-01-17 06:01:03.719383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "2023-01-17 06:01:05.673994: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2899885000 Hz\n",
      "2023-01-17 06:01:05.674597: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2063a10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-17 06:01:05.674613: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-17 06:01:05.675829: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n"
     ]
    }
   ],
   "source": [
    "!classification_tf1 train -e $SPECS_DIR/classification_retrain_spec.cfg \\\n",
    "                      -r $EXPERIMENT_DIR/output_retrain \\\n",
    "                      -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ~/.nv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao classification_tf1 evaluate -e $SPECS_DIR/classification_retrain_spec.cfg -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2023-01-16 14:57:32.584614: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "2023-01-16 14:57:35.550715: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2899885000 Hz\n",
      "2023-01-16 14:57:35.551336: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x37b6990 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 14:57:35.551353: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-01-16 14:57:35.553586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-16 14:57:35.602486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:57:35.602656: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x615ef20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-16 14:57:35.602672: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2023-01-16 14:57:35.603010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:57:35.603190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1669] Found device 0 with properties: \n",
      "name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\n",
      "pciBusID: 0000:01:00.0\n",
      "2023-01-16 14:57:35.603215: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 14:57:35.669407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-16 14:57:35.677449: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-16 14:57:35.679015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-16 14:57:35.680737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-01-16 14:57:35.684355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-16 14:57:35.684464: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-16 14:57:35.684540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:57:35.684659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:57:35.684721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1797] Adding visible gpu devices: 0\n",
      "2023-01-16 14:57:35.684913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-16 14:57:36.631719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1209] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-16 14:57:36.631746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1215]      0 \n",
      "2023-01-16 14:57:36.631752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1228] 0:   N \n",
      "2023-01-16 14:57:36.632085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:57:36.632236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1082] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 14:57:36.632318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1354] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4647 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "/usr/local/lib/python3.6/dist-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "/usr/local/lib/python3.6/dist-packages/numba/cuda/envvars.py:17: NumbaWarning: \n",
      "Environment variables with the 'NUMBAPRO' prefix are deprecated and consequently ignored, found use of NUMBAPRO_LIBDEVICE=/usr/local/cuda/nvvm/libdevice/.\n",
      "\n",
      "For more information about alternatives visit: ('http://numba.pydata.org/numba-doc/latest/cuda/overview.html', '#cudatoolkit-lookup')\n",
      "  warnings.warn(errors.NumbaWarning(msg))\n",
      "2023-01-16 14:57:41,250 [INFO] iva.common.export.keras_exporter: Using input nodes: ['input_1']\n",
      "2023-01-16 14:57:41,250 [INFO] iva.common.export.keras_exporter: Using output nodes: ['predictions/Softmax']\n",
      "NOTE: UFF has been tested with TensorFlow 1.14.0.\n",
      "WARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.\n",
      "DEBUG: convert reshape to flatten node\n",
      "DEBUG [/usr/local/lib/python3.6/dist-packages/uff/converters/tensorflow/converter.py:96] Marking ['predictions/Softmax'] as outputs\n",
      "2023-01-16 14:57:49,296 [INFO] iva.common.export.base_exporter: Generating a tensorfile with random tensor images. This may work well as a profiling tool, however, it may result in inaccurate results at inference. Please generate a tensorfile using the tlt-int8-tensorfile, or provide a custom directory of images for best performance.\n",
      "100%|███████████████████████████████████████████| 10/10 [00:00<00:00, 10.77it/s]\n",
      "2023-01-16 14:57:50,226 [INFO] iva.common.export.keras_exporter: Calibration takes time especially if number of batches is large.\n",
      "2023-01-16 14:58:05,095 [INFO] iva.common.export.base_calibrator: Saving calibration cache (size 3928) to /root/TAO/results/export/final_model_int8_cache.bin\n",
      "Telemetry data couldn't be sent, but the command ran successfully.\n",
      "[WARNING]: <urlopen error [Errno -2] Name or service not known>\n",
      "Execution status: PASS\n"
     ]
    }
   ],
   "source": [
    "!classification_tf1 export \\\n",
    "        -m $EXPERIMENT_DIR/output/weights/resnet_010.tlt \\\n",
    "        -o $EXPERIMENT_DIR/export/final_model.etlt \\\n",
    "        -k $KEY \\\n",
    "        --cal_data_file $EXPERIMENT_DIR/export/calibration.tensor \\\n",
    "        --data_type int8 \\\n",
    "        --batches 10 \\\n",
    "        --cal_cache_file $EXPERIMENT_DIR/export/final_model_int8_cache.bin \\\n",
    "        --classmap_json $EXPERIMENT_DIR/output_retrain/classmap.json \\\n",
    "        --gen_ds_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [MemUsageChange] Init CUDA: CPU +204, GPU +0, now: CPU 216, GPU 795 (MiB)\n",
      "[INFO] [MemUsageChange] Init builder kernel library: CPU +121, GPU +22, now: CPU 392, GPU 820 (MiB)\n",
      "[WARNING] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
      "[WARNING] The implicit batch dimension mode has been deprecated. Please create the network with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag whenever possible.\n",
      "[INFO] Reading Calibration Cache for calibrator: EntropyCalibration2\n",
      "[INFO] Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales.\n",
      "[INFO] To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache.\n",
      "[WARNING] Missing scale and zero-point for tensor conv1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor bn_conv1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1a_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_1b_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2a_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_2b_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3a_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_3b_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4a_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_conv_1/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_1/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_conv_2/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_2/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_conv_shortcut/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/moving_variance, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/Reshape_1/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/batchnorm/add/y, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/gamma, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/Reshape_3/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/beta, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/Reshape_2/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/moving_mean, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor block_4b_bn_shortcut/Reshape/shape, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor predictions/kernel, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[WARNING] Missing scale and zero-point for tensor predictions/bias, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
      "[INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +192, GPU +80, now: CPU 717, GPU 901 (MiB)\n",
      "[INFO] [MemUsageChange] Init cuDNN: CPU +111, GPU +56, now: CPU 828, GPU 957 (MiB)\n",
      "[INFO] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[INFO] Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.\n",
      "[INFO] Total Activation Memory: 1124823552\n",
      "[INFO] Detected 1 inputs and 1 output network tensors.\n",
      "[INFO] Total Host Persistent Memory: 51424\n",
      "[INFO] Total Device Persistent Memory: 52224\n",
      "[INFO] Total Scratch Memory: 0\n",
      "[INFO] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 57 MiB, GPU 771 MiB\n",
      "[INFO] [BlockAssignment] Started assigning block shifts. This will take 29 steps to complete.\n",
      "[INFO] [BlockAssignment] Algorithm ShiftNTopDown took 0.168104ms to assign 3 blocks to 29 nodes requiring 15360000 bytes.\n",
      "[INFO] Total Activation Memory: 15360000\n",
      "[INFO] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1166, GPU 1120 (MiB)\n",
      "[WARNING] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[WARNING] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[WARNING] Check verbose logs for the list of affected weights.\n",
      "[WARNING] - 26 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[WARNING] - 11 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[INFO] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +11, GPU +12, now: CPU 11, GPU 12 (MiB)\n"
     ]
    }
   ],
   "source": [
    "!converter $EXPERIMENT_DIR/export/final_model.etlt \\\n",
    "           -k nvidia_tao \\\n",
    "           -c $EXPERIMENT_DIR/export/final_model_int8_cache.bin \\\n",
    "           -o predictions/Softmax \\\n",
    "           -d 3,100,100 \\\n",
    "           -i nchw \\\n",
    "           -m 64 -t int8 \\\n",
    "           -e $EXPERIMENT_DIR/export/final_model.trt \\\n",
    "           -b 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
